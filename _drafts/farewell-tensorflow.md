---
layout: post
title: Farewell, TensorFlow
excerpt-separator: <!--more-->
author:
- Derek Murray
---

Friday was my last day working at Google on [TensorFlow](https://www.tensorflow.org/). The past five years were a lot of fun, and I feel incredibly lucky to have been in the war room at 6 AM when we [launched the project into the world](https://bits.blogs.nytimes.com/2015/11/09/google-offers-free-software-in-bid-to-gain-an-edge-in-machine-learning/). Since then, it's been inspiring to see how people have used TensorFlow, from all the folks [asking questions on Stack Overflow](https://stackoverflow.com/users/3574081/mrry), through the machine learning and systems [research that built on our work](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13312035063239472247,4870469586968585222,14846780091870890862), to the huge [software ecosystem](https://twitter.com/DynamicWebPaige/status/1065452146924040192) that has grown up around it.

<!--more-->

Before I move on, I wanted to reminisce about what I enjoyed most about working at Google. Google is [well-known](https://www.amazon.com/Software-Engineering-Google-Lessons-Programming/dp/1492082791) for the in-house developer infrastructure that its software engineers use every day. Among my favorites as a curious engineer, [Code Search](https://opensource.googleblog.com/2020/04/code-search-for-google-open-source.html) and [Piper](https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext) offer a way to learn how almost anything the company uses was implemented, including several systems whose papers have been required reading in graduate-level systems classes for more than a decade (and even the edit histories for some of those papers).

For me, however, the most vital developer infrastructure was the [Google Wide Profiler (GWP)](https://research.google/pubs/pub36575/). GWP aggregates profiles from all code running in production, so you can understand how it performs under realistic conditions. Once some code becomes heavily used, it starts showing up in GWP, letting you measure the impact of performance optimizations in terms of thousands of CPUs saved, or more. Sometimes a straightforward C++ change, like adding a `std::move()`, or switching a `std::string` argument to take a [`std::string_view`](https://en.cppreference.com/w/cpp/string/basic_string_view), will save a few hundred cores.[^1] In terms of effort-to-impact ratio, [this single character change to the `tf.concat()` implementation](https://github.com/tensorflow/tensorflow/commit/2c130a0d252741d2c0ce1e0d815ac1d87d53d6e3#diff-ba035a495fc7daf4cd233f428921bd96) will be hard to beat. But GWP can also provoke more in-depth investigations, and I wanted to share the story of my final significant contribution to TensorFlow.

## A GWP story: optimizing TensorFlow's executor

Most mornings, I would check in with GWP to see what functions in TensorFlow were hot over the last day, and find if there were any surprising new entries or regressions. When I did this one day last month, a familiar symbol stared back at me: [`tensorflow::(anonymous namespace)::ExecutorState::PropagateOutputs()`](https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/core/common_runtime/executor.cc#L2174-L2281). This method is the heart of TensorFlow's dataflow executor, and it is responsible for two things: (i) forwarding output tensors from one kernel to the kernels that consume them, and (ii) adding kernels to the scheduler queue when they become runnable. In other words, it is literally **the method that makes tensors flow in TensorFlow**. It was also painfully inefficient, and I didn't want to leave it as unfinished business.

For sure, `PropagateOutputs()` is non-trivial code, and [better engineers than I have optimized it](https://github.com/tensorflow/tensorflow/commit/de94a13ce98f2477d890cae02a6ea3846a9ba2b7) down to the level of structure packing and bitfields. Even with these optimizations, it remained stubbornly expensive, made worse by the [contended mutex that guards the state update](https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/core/common_runtime/executor.cc#L2206). This mutex would make most kernel completions within a single graph run sequentially, and put worker threads to sleep when they still have a lot of work to do. This was particularly bad when you have a lot of fine-grained and potentially parallelizable kernels in your graph, which is common in inference and `tf.data` input preprocessing.

At a high level, the logic in `PropagateOutputs()` is simple, based on [Kahn's algorithm for topological sorting](https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm):

1. Each kernel starts out with a "pending count" equal to its in-degree.
2. When a kernel finishes, it forwards its outputs along edges in the graph, and decrements the pending count for each consumer kernel.
3. When the pending count for a kernel hits zero, it becomes runnable, and we add it to the ready queue.

Unfortunately, it's a bit more complicated than that: the propagation rules are different if the consumer is a special control flow kernel, or if the edge is in a dead branch of a control flow construct (e.g. the untaken branch of a [`tf.cond()`](https://www.tensorflow.org/api_docs/python/tf/cond?hl=en)).[^2] As a result, `PropagateOutputs()` had to load the graph structure for the consumer kernels, which was unlikely to be in the cache already, and compute hard-to-predict branches on it. On top of this overhead, the mutex in `PropagateOutputs()` had been bothering me for a long time. It always seemed possible to model the "pending count" as an [atomic reference count](https://en.wikipedia.org/wiki/Reference_counting) and remove the mutex altogether for some graphs. However, the code is intricate, and so heavily used that one false step changing it could easily cost more than my salary in increased utilization.

The answer seemed clear: we need one executor for graphs with control flow, and another simpler executor for graphs without it. The original release of TensorFlow actually had this split, but it was too tedious to keep the duplicated logic in sync, so [they were combined](https://github.com/tensorflow/tensorflow/commit/96aa23e45e72bd765920cc957090ff453e25cf64). To land the optimization, I needed a zero-cost way to dispatch to different implementations of `PropagateOutputs()` based on the static graph topology: even making `PropagateOutputs()` a virtual method would slow down existing users too much. Ultimately, I came up with the following steps that achieved my goal without too much duplication:

1. [Split](https://github.com/tensorflow/tensorflow/commit/bd530a65d5712b0734c0b6c9af5aa83ccd9e7387) the propagation and execution logic into two classes, `PropagatorState` and `ExecutorState<PropagatorStateType>`.
2. [Implement](https://github.com/tensorflow/tensorflow/commit/4c36ade963b0a7f1c8d2ea480d5b8c0922f6bebf) a `SimplePropagatorState` class with the same interface as `PropagatorState`, and [instantiate an `ExecutorState<SimplePropagatorState>` when the graph contains no control flow](https://github.com/tensorflow/tensorflow/commit/4c36ade963b0a7f1c8d2ea480d5b8c0922f6bebf#diff-f0761b2f596c3251df04d000e21d4cf9R1272-R1274). The `SimplePropagatorState` avoids unnecessary indirections, dynamic allocations, and branches when running simple graphs.
3. [Replace](https://github.com/tensorflow/tensorflow/commit/4b05562d485600e824f828e089f21646bf3efcc4) the `PendingCounts` in `SimplePropagatorState` with [atomic counts for each kernel](https://github.com/tensorflow/tensorflow/commit/4b05562d485600e824f828e089f21646bf3efcc4#diff-c30dd4145e8b858ce581e36cad94c1d9R99-R101), and finally [get rid of the infernal mutex](https://github.com/tensorflow/tensorflow/commit/4b05562d485600e824f828e089f21646bf3efcc4#diff-c30dd4145e8b858ce581e36cad94c1d9L68).

Along the way, I relied on a [growing suite of microbenchmarks](https://github.com/tensorflow/tensorflow/blob/de5b0cfd434c7a9f848ab100b70a6be16e48280b/tensorflow/core/common_runtime/executor_test.cc) to ensure that my changes didn't slow down existing users. For a general-purpose component like the executor, it can be hard to predict what benchmarks will be useful: measurements from GWP were critical in ensuring that the microbenchmarks tracked the realistic usage patterns, which were not always easy for the team to predict. The microbenchmarks contain a suite of synthetic graphs, and, combined with [`pprof`](https://github.com/google/pprof), they gave me an easy way to visualize executor performance as a [flamegraph](http://www.brendangregg.com/flamegraphs.html) or drill down to the time spent in individual instructions.

The results so far have been encouraging: as days went by, GWP showed the fraction of time spent in `PropagateOutputs()` decrease as users rebuilt their code with the latest version. There were some decent reductions to inference latency, with some users reporting up to a 10% improvement end-to-end.  If you want to try out the new code, the changes will be in the upcoming 2.3 release, or they are available today [on GitHub](https://github.com/tensorflow/tensorflow/) and in [tf-nightly](https://pypi.org/project/tf-nightly/). There are still opportunities to improve things: in particular, it feels like it should be possible to extend the atomic optimization to at least some graphs that have control flow (at least `tf.cond()`, if not `tf.while_loop()`). If you see something that could be done better, I hope you'll consider [submitting a pull request](https://github.com/tensorflow/tensorflow/pulls)!

[^1]: Many of these can be found automatically with tools like [`clang-tidy`](https://clang.llvm.org/extra/clang-tidy/).

[^2]: We wrote [a paper](https://dl.acm.org/doi/10.1145/3190508.3190551) about TensorFlow's control flow scheme, but unfortunately it doesn't go into detail about the fine details of efficient executor implementations. With hindsight, we should have evaluated the effect on performance of adding control flow support to graphs without control flow.
